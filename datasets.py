import osimport reimport pickleimport structimport sysimport numpy as npimport tensorflow as tfimport configfrom core_helpers import image_preprocessing"""     Dictionary of used data sets."""data_set_root = os.path.join(os.getcwd(), 'data')FLAGS = tf.app.flags.FLAGSdef find_txt(folder):    """        Finds *txt file in folder content        :param folder: A folder data directory    """    txt_files = []    if config.platform == 'win32':        pattern = r"(?<=\\)\w+(?=.txt)"    else:        pattern = r"(?<=/)\w+(?=.txt)"    for file in folder:        if len(re.findall(pattern, file)) > 0:            txt_files.append(file)    return txt_filesdef find_train(files):    """        Finds files including "train" in name.        :param files: file name        :return: train file    """    if config.platform == 'win32':        pattern = r"(?<=\\)(?=train)\w."    else:        pattern = r"(?<=/)(?=train)\w."    for file in files:        if len(re.findall(pattern, file)) > 0:            return filedef find_test(files):    """        Finds files including "test" in name.        :param files: file name        :return test file    """    for file in files:        if len(re.findall(r"(?<=/)(?=test)\w.", file)) > 0:            return filedef fetch_mtfl_debug(data_set, volume):    data_files = find_txt(data_set.files)    training_path = find_train(data_files)    # Fetch train for debug    train_file = open(training_path, 'r')    train_data = {"file_path": [], "landmarks": [], "poses": []}    test_data = {"file_path": [], "landmarks": [], "poses": []}    for i, line in enumerate(train_file):        if i < volume:            t = line.split()            train_data["file_path"].append(os.path.join(data_set.path, t[0].replace("\\", "/")))            train_data["landmarks"].append([float(l) for l in t[1:11]])            train_data["poses"].append([int(p) for p in (t[11:15])])    # Split 2 instances for test    test_data["file_path"] = train_data["file_path"][-2:]    test_data["landmarks"] = train_data["landmarks"][-2:]    test_data["poses"] = train_data["poses"][-2:]    return {"train": train_data, "test": test_data}def fetch_mtfl(data_set):    data_files = find_txt(data_set.files)    training_path = find_train(data_files)    test_path = find_test(data_files)    # Fetch train    train_file = open(training_path, 'r')    train_data = {"file_path": [], "landmarks": [], "poses": []}    for line in train_file:        t = line.split()        train_data["file_path"].append(os.path.join(data_set.path, t[0].replace("\\", "/")))        train_data["landmarks"].append([float(l) for l in t[1:11]])        train_data["poses"].append([int(p) for p in (t[11:15])])    # Fetch test    test_file = open(test_path, 'r')    test_data = {"file_path": [], "landmarks": [], "poses": []}    for line in test_file:        t = line.split()        test_data["file_path"].append(os.path.join(data_set.path, t[0].replace("\\", "/")))        test_data["landmarks"].append([float(l) for l in t[1:11]])        test_data["poses"].append([int(p) for p in (t[11:15])])    return {"train": train_data, "test": test_data}def route(prefix):    class DataSetEntity(object):        pass    data_set = DataSetEntity()    folder_content = []    if prefix.lower() == 'mtfl':        """             Data set on Multi Task Deep Learning for Facial Landmark Detection            @ http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html        """        data_set.tag = 'MTFL'        data_set.path = os.path.join(data_set_root, data_set.tag)        exclude_test = False    elif prefix.lower() == 'mtfl-batches-bin':        """            Data set on Multi Task Deep Learning for Facial Landmark Detection            created as bins        """        data_set.tag = "MTFL-batches-bin"        data_set.path = os.path.join(data_set_root, data_set.tag)        exclude_test = True    # Also add file paths in dir    content = os.listdir(data_set.path)    for file in content:        # exclude system hidden files        if re.match("^[.]", file) == None:            folder_content.append(os.path.join(data_set.path, file))    if exclude_test:        # Sort them        folder_content.sort()        # Exclude test batch in train batches        data_set.files = folder_content[:-1]    else:        data_set.files = folder_content    return data_setdef bring(data_set_name, volume=None):    if data_set_name == 'MTFL':        data = fetch_mtfl(route('MTFL'))    elif data_set_name == 'MTFL_DEBUG':        """ Mtfl for debug purposes"""        data = fetch_mtfl_debug(route('MTFL'), volume)    elif data_set_name == 'MTFL-batches-bin':        """ MTFL batches """        data = route('MTFL-batches-bin')    return datadef write_to_binary(data_set_tag):    """    :param data_set_tag:    :return:    """    # Check if bin folder already exists, if it does break the program    if os.path.exists(os.path.join(FLAGS.bin_path, "data_batch_1.bin")):        print("Bins already exists")        print("Going back")    else:        # Check if log folder exists, unless create        if not os.path.exists(FLAGS.log_dir):            os.makedirs(FLAGS.log_dir)        # Create a log file.        with open(os.path.join(FLAGS.log_dir, "write_to_binary_logs.txt"), "w") as log_file:            print("Writing to binary: started")            # dataset = bring("MTFL_DEBUG", 10)            dataset = bring(data_set_tag)            dataset = image_preprocessing.read_images_and_combine_others(dataset, image_size=FLAGS.bin_image_size)            # Normalization wont be held in here:            # uar_dataset = image_preprocessing.uniform_aspect_ratio(dataset)            # rszd_dataset = image_preprocessing.force_to_resize(uar_dataset)            # rszd_dataset = image_preprocessing.force_to_resize(uar_dataset)            # zs_dataset = image_preprocessing.zero_slope(rszd_dataset)            # alligned_dataset = image_preprocessing.face_allignment(zs_dataset)            # rszd_std_dataset = image_preprocessing.resize_to_std(alligned_dataset, image_size=40)            # data = {"image": rszd_std_dataset["train"]["image"] + rszd_std_dataset["test"]["image"],            #         "pose": rszd_std_dataset["train"]["pose"] + rszd_std_dataset["test"]["pose"],            #         "landmark": rszd_std_dataset["train"]["landmark"] + rszd_std_dataset["test"]["landmark"]}            # data_bin_size = 10014*2000            # test_bin_size = 10014*2995            # @ todo: you may want to make this numbers dataset specific            n_of_train_bins = 10            n_of_test_bins = 1            train_per_bin_size = len(dataset['train']['file_path']) / n_of_train_bins  # 200 vs 3073x 10000            test_per_bin_size = len(dataset['test']['file_path']) / n_of_test_bins            data_bin_size = int((10 + 4 + FLAGS.bin_image_size * FLAGS.bin_image_size) * train_per_bin_size)            test_bin_size = int((10 + 4 + FLAGS.bin_image_size * FLAGS.bin_image_size) * test_per_bin_size)            bin_chunks = [{"name": "data_batch_1.bin", "size": data_bin_size},                          {"name": "data_batch_2.bin", "size": data_bin_size},                          {"name": "data_batch_3.bin", "size": data_bin_size},                          {"name": "data_batch_4.bin", "size": data_bin_size},                          {"name": "data_batch_5.bin", "size": data_bin_size},                          {"name": "data_batch_6.bin", "size": data_bin_size},                          {"name": "data_batch_7.bin", "size": data_bin_size},                          {"name": "data_batch_8.bin", "size": data_bin_size},                          {"name": "data_batch_9.bin", "size": data_bin_size},                          {"name": "data_batch_10.bin", "size": data_bin_size},                          {"name": "test_batch.bin", "size": test_bin_size}]            if not os.path.exists(FLAGS.bin_path):                os.makedirs(FLAGS.bin_path)            # Create train batches            print("Creating train batches")            byte_tracker = 0            for value in bin_chunks[:-1]:                # Train binary file:                with open(os.path.join(FLAGS.bin_path, value["name"]), "wb") as bin_file:                    for i, image in enumerate(dataset["train"]["image"]):                        print("Train: " + str(i + 1))                        list_to_byte = []                        # The first 4 bytes are labels are: gender, smile, glasses, head pose                        list_to_byte.extend(dataset["train"]["pose"][i])                        # The next 10 bytes are landmarks                        list_to_byte.extend([int(l) for l in dataset["train"]["landmark"][i]])                        # in case of two channel                        # Convert the image into numpy array and reshape it                        np_image_reshaped = (np.array(image)).reshape((FLAGS.bin_image_size * FLAGS.bin_image_size, 1))                        list_to_byte.extend(int(pixels[0]) for pixels in np_image_reshaped)                        # in case of three channel                        # r, g, b = image.split()                        # # The next 10000 bytes are red pixels                        # list_to_byte.extend([int(r_pixels) for r_pixels in list(r.getdata())])                        # # The next 10000 bytes are green pixels                        # list_to_byte.extend([int(g_pixels) for g_pixels in list(g.getdata())])                        # # The next 10000 bytes are blue pixels                        # list_to_byte.extend([int(b_pixels) for b_pixels in list(b.getdata())])                        byte_to_write = bytearray(list_to_byte)                        byte_tracker += len(list_to_byte)                        if byte_tracker < value["size"]:                            bin_file.write(byte_to_write)                            log_file.write("Train instance " + str(i + 1) + " :Success to write bin\n")                        elif byte_tracker == value["size"]:                            bin_file.write(byte_to_write)                            bin_file.close()                            byte_tracker = 0                            log_file.write("Train instance " + str(i + 1) + " :is end of the batch\n")                            log_file.write("-------End of the batch-----------------------------\n")                            break            del byte_tracker, bin_file, list_to_byte, byte_to_write, np_image_reshaped, i, image            # Create test batches            print("Creating test batches")            byte_tracker = 0            for value in [bin_chunks[-1]]:                # Train binary file:                with open(os.path.join(FLAGS.bin_path, value["name"]), "wb") as bin_file:                    for i, image in enumerate(dataset["test"]["image"]):                        print("Test: " + str(i + 1))                        list_to_byte = []                        # The first 4 bytes are labels are: gender, smile, glasses, head pose                        list_to_byte.extend(dataset["test"]["pose"][i])                        # The next 10 bytes are landmarks                        list_to_byte.extend([int(l) for l in dataset["train"]["landmark"][i]])                        # in case of two channel                        # Convert the image into numpy array and reshape it                        np_image_reshaped = (np.array(image)).reshape((FLAGS.bin_image_size * FLAGS.bin_image_size, 1))                        list_to_byte.extend(int(pixels[0]) for pixels in np_image_reshaped)                        # in case of three channel                        # r, g, b = image.split()                        # # The next 10000 bytes are red pixels                        # list_to_byte.extend([int(r_pixels) for r_pixels in list(r.getdata())])                        # # The next 10000 bytes are green pixels                        # list_to_byte.extend([int(g_pixels) for g_pixels in list(g.getdata())])                        # # The next 10000 bytes are blue pixels                        # list_to_byte.extend([int(b_pixels) for b_pixels in list(b.getdata())])                        byte_to_write = bytearray(list_to_byte)                        byte_tracker += len(list_to_byte)                        if byte_tracker < value["size"]:                            bin_file.write(byte_to_write)                            log_file.write("Test instance " + str(i + 1) + " :Success to write bin\n")                        elif byte_tracker == value["size"]:                            bin_file.write(byte_to_write)                            bin_file.close()                            byte_tracker = 0                            log_file.write("Test instance " + str(i + 1) + " :is end of the batch\n")                            log_file.write("-------End of the batch-----------------------------\n")                            break        print("Writing to binary: done")        print("Bins are ready")